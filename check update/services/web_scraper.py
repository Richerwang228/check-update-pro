import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from typing import Dict, List, Optional
import logging
from urllib.parse import urljoin, urlparse
import time
import random
from collections import defaultdict
import urllib3
import uuid
import hashlib

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å¯¼å…¥è¯·æ±‚ç®¡ç†å™¨å’Œç¼“å­˜
from services.request_manager import request_manager
from utils.page_cache import page_cache
from config.settings import PAGE_CACHE_TTL

# å¯¼å…¥é…ç½®
try:
    from config.anti_ban_config import AntiBanConfig
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    class AntiBanConfig:
        MIN_INTERVAL = 2
        MAX_INTERVAL = 15
        MAX_RETRIES = 5
        PROXY_POOL = [None]
        USER_AGENTS = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36']

class WebScraper:
    def __init__(self):
        self.session = requests.Session()
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
        self.session.headers.update(AntiBanConfig.HEADERS)
        self.proxies = AntiBanConfig.PROXY_POOL
        self.user_agents = AntiBanConfig.USER_AGENTS
        
        self.current_proxy_index = 0
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        
        self.domain_stats = defaultdict(lambda: {'last_request': 0, 'current_interval': 1, 'consecutive_failures': 0})
        self.max_interval = AntiBanConfig.MAX_INTERVAL
        self.min_interval = AntiBanConfig.MIN_INTERVAL
        self.failure_penalty = AntiBanConfig.FAILURE_PENALTY
        self.domain_min_length = {
            'hsex.men': 300
        }
        
    def _setup_cookies(self):
        """è®¾ç½®æ›´å®Œæ•´çš„æµè§ˆå™¨Cookieæ¨¡æ‹Ÿ"""
        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID
        session_id = str(uuid.uuid4()).replace('-', '')[:26]
        cf_clearance = ''.join(random.choices('0123456789abcdef', k=43))
        cf_bm = ''.join(random.choices('0123456789abcdef', k=30))
        
        # è®¾ç½®hsex.menç›¸å…³çš„Cookie
        self.session.cookies.set('PHPSESSID', session_id, domain='hsex.men')
        self.session.cookies.set('cf_clearance', cf_clearance, domain='.hsex.men')
        self.session.cookies.set('__cf_bm', cf_bm, domain='.hsex.men')
        self.session.cookies.set('_ga', f'GA1.2.{random.randint(1000000000, 9999999999)}.{int(time.time())}', domain='.hsex.men')
        self.session.cookies.set('_gid', f'GA1.2.{random.randint(100000000, 999999999)}', domain='.hsex.men')
        self.session.cookies.set('_gat', '1', domain='.hsex.men')
        
        # è®¾ç½®é€šç”¨Cookie
        self.session.cookies.set('timezone', 'Asia/Shanghai')
        self.session.cookies.set('language', 'zh-CN')
        
    def _run_network_diagnosis(self, url: str):
        """è¿è¡Œç½‘ç»œè¯Šæ–­ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£é—®é¢˜"""
        self.logger.info("ğŸ” æ­£åœ¨è¿è¡Œå¿«é€Ÿç½‘ç»œè¯Šæ–­...")
        print("ğŸ” æ­£åœ¨è¿è¡Œå¿«é€Ÿç½‘ç»œè¯Šæ–­...")
        print("=" * 50)
        
        # æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥
        try:
            import socket
            hostname = 'hsex.men'
            port = 443
            
            # DNSè§£ææµ‹è¯•
            try:
                ip = socket.gethostbyname(hostname)
                print(f"âœ… DNSè§£ææˆåŠŸ: {hostname} -> {ip}")
            except socket.gaierror:
                print(f"âŒ DNSè§£æå¤±è´¥: æ— æ³•è§£æ {hostname}")
                print("ğŸ’¡ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ›´æ¢DNSæœåŠ¡å™¨")
                return
                
            # ç«¯å£è¿æ¥æµ‹è¯•
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(5)
                result = sock.connect_ex((hostname, port))
                sock.close()
                
                if result == 0:
                    print(f"âœ… ç«¯å£è¿æ¥æˆåŠŸ: {hostname}:{port}")
                else:
                    print(f"âŒ ç«¯å£è¿æ¥å¤±è´¥: {hostname}:{port} (é”™è¯¯ç : {result})")
                    print("ğŸ’¡ å»ºè®®: æ£€æŸ¥é˜²ç«å¢™è®¾ç½®æˆ–ç½‘ç»œé™åˆ¶")
                    return
                    
            except Exception as e:
                print(f"âŒ ç«¯å£æµ‹è¯•å¼‚å¸¸: {str(e)}")
                
        except Exception as e:
            print(f"âŒ ç½‘ç»œè¯Šæ–­å¼‚å¸¸: {str(e)}")
            
        # ä»£ç†çŠ¶æ€æ£€æŸ¥ï¼ˆä»…å¿«é€Ÿæ£€æŸ¥ï¼Œä¸æµ‹è¯•è¿æ¥ï¼‰
        working_proxies = []
        has_proxy = False
        for proxy in self.proxies:
            if proxy is None:
                working_proxies.append(proxy)
            else:
                has_proxy = True
                working_proxies.append(proxy)
        
        self.proxies = working_proxies if working_proxies else [None]
        
        print("\nğŸ“Š è¯Šæ–­ç»“æœ:")
        if not has_proxy:
            print("ğŸ“¡ ä½¿ç”¨ç›´è¿æ¨¡å¼")
        else:
            print(f"âœ… é…ç½®äº† {len([p for p in self.proxies if p])} ä¸ªä»£ç†")
            
        print("=" * 50)
        self.logger.info("âœ“ ç½‘ç»œè¯Šæ–­å®Œæˆ")

    def _get_domain(self, url: str) -> str:
        return urlparse(url).netloc

    def _adjust_interval(self, domain: str, success: bool):
        stats = self.domain_stats[domain]
        if success:
            stats['current_interval'] = max(self.min_interval, stats['current_interval'] * 0.5)
            stats['consecutive_failures'] = 0
        else:
            stats['consecutive_failures'] += 1
            penalty = self.failure_penalty ** min(stats['consecutive_failures'], 3)
            stats['current_interval'] = min(self.max_interval, stats['current_interval'] * penalty)

    def is_valid_html_for_domain(self, domain: str, html: str) -> bool:
        if domain.endswith('hsex.men'):
            patterns = [
                r'class=[\"\']?[^>]*col-xs-6\s+col-md-3',
                r'class=[\"\']?[^>]*thumbnail',
                r'class=[\"\']?[^>]*video-item',
                r'class=[\"\']?[^>]*item',
                r'class=[\"\']?[^>]*card',
                r'class=[\"\']?[^>]*video-card'
            ]
            for p in patterns:
                if re.search(p, html):
                    return True
        flags = [
            'cloudflare',
            'just a moment',
            'enable javascript',
            'checking your browser'
        ]
        low = html.lower()
        for f in flags:
            if f in low:
                return False
        return False

    def get_min_length_for_domain(self, domain: str) -> int:
        return self.domain_min_length.get(domain, 500)

    def get_page_content(self, url: str, max_retries: int = None, use_cache: bool = True) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹ï¼ˆæ”¯æŒç¼“å­˜å’Œæ™ºèƒ½é‡è¯•ï¼‰
        
        Args:
            url: é¡µé¢URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            HTMLå†…å®¹æˆ–None
        """
        if max_retries is None:
            max_retries = AntiBanConfig.MAX_RETRIES
        
        if use_cache:
            cached = page_cache.get_with_meta(url)
            if cached:
                cached_html, meta = cached
            else:
                cached_html, meta = None, {}
        
        # 2. ç½‘ç»œè¯Šæ–­æ¨¡å¼ - ä»…åœ¨debugæ¨¡å¼è¿è¡Œ
        # if not hasattr(self, '_diagnosed'):
        #     self._diagnosed = True
        #     self._run_network_diagnosis(url)
        
        domain = self._get_domain(url)
        stats = self.domain_stats[domain]
        
        # 3. ä½¿ç”¨è¯·æ±‚ç®¡ç†å™¨æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…
        request_manager.wait_if_needed(domain)
        
        # Cloudflareæ£€æµ‹æ¨¡å¼
        cloudflare_detected = False
        
        short_content_streak = 0
        force_no_cache = False
        for attempt in range(max_retries):
            try:
                # è®¾ç½®Cookie
                self._setup_cookies()
                
                # åŠ¨æ€è®¾ç½®è¯·æ±‚å¤´
                headers = dict(self.session.headers)
                headers['Referer'] = f"https://{domain}/"
                headers['User-Agent'] = random.choice(self.user_agents)
                
                # æ·»åŠ æ›´çœŸå®çš„æµè§ˆå™¨æŒ‡çº¹
                headers['sec-ch-ua'] = '"Chromium";v="128", "Not;A=Brand";v="24", "Google Chrome";v="128"'
                headers['sec-ch-ua-mobile'] = '?0'
                headers['sec-ch-ua-platform'] = '"Windows"'
                if force_no_cache:
                    headers['Cache-Control'] = 'no-cache'
                    headers['Pragma'] = 'no-cache'
                
                # é’ˆå¯¹Cloudflareçš„ç‰¹æ®Šå¤„ç†
                if cloudflare_detected:
                    # é‡åˆ°Cloudflareæ—¶å¤§å¹…å¢åŠ ç­‰å¾…æ—¶é—´
                    wait_time = request_manager.get_retry_delay(domain, attempt) * 2
                    self.logger.warning(f"Cloudflareæ£€æµ‹åˆ°ï¼Œç­‰å¾…{wait_time:.1f}ç§’...")
                    time.sleep(wait_time)
                elif attempt > 0:
                    # ä½¿ç”¨è¯·æ±‚ç®¡ç†å™¨çš„æ™ºèƒ½é‡è¯•å»¶è¿Ÿ
                    retry_delay = request_manager.get_retry_delay(domain, attempt)
                    self.logger.info(f"é‡è¯• {attempt+1}/{max_retries}ï¼Œç­‰å¾… {retry_delay:.1f} ç§’")
                    time.sleep(retry_delay)
                
                # é€‰æ‹©ä»£ç† - ä¼˜å…ˆä½¿ç”¨ç›´è¿
                current_proxy = None if attempt == 0 else self.proxies[self.current_proxy_index % len(self.proxies)]
                
                if use_cache and cached_html:
                    if 'etag' in meta:
                        headers['If-None-Match'] = meta['etag']
                    if 'last_modified' in meta:
                        headers['If-Modified-Since'] = meta['last_modified']
                request_manager.enter_request(domain)
                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=current_proxy,
                    timeout=(10, 30),
                    allow_redirects=True,
                    verify=False
                )
                stats['last_request'] = time.time()
                if use_cache and response.status_code == 304 and cached_html:
                    request_manager.record_request(domain, True)
                    if use_cache:
                        page_cache.set(url, cached_html, {
                            'etag': response.headers.get('ETag', meta.get('etag', '')),
                            'last_modified': response.headers.get('Last-Modified', meta.get('last_modified', ''))
                        })
                    self.logger.info(f"âœ“ ç¼“å­˜æœªè¿‡æœŸ: {url[:50]}...")
                    request_manager.exit_request(domain)
                    return cached_html
                
                # æ£€æµ‹Cloudflare
                is_cloudflare = (
                    response.status_code == 403 or
                    "cloudflare" in response.text.lower() or
                    "just a moment" in response.text.lower() or
                    "ray id" in response.text.lower() or
                    "enable javascript" in response.text.lower() or
                    "checking your browser" in response.text.lower()
                )
                
                # å¤„ç†CloudflareéªŒè¯
                if is_cloudflare:
                    cloudflare_detected = True
                    self.logger.warning("ğŸ›¡ï¸  æ£€æµ‹åˆ°Cloudflareä¿æŠ¤")
                    
                    # å¦‚æœæ˜¯é¦–æ¬¡é‡åˆ°ï¼Œç»™å‡ºå…·ä½“å»ºè®®
                    if attempt == 0:
                        print("\n" + "="*60)
                        print("ğŸš¨ Cloudflareé˜²æŠ¤æ£€æµ‹åˆ°")
                        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                        print("   1. ä½¿ç”¨ä»˜è´¹ä»£ç†æœåŠ¡ (æ¨è: Bright Dataä½å®…ä»£ç†)")
                        print("   2. é™ä½è¯·æ±‚é¢‘ç‡ (ç­‰å¾…30-60ç§’)")
                        print("   3. ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒ (Selenium/Playwright)")
                        print("   4. è€ƒè™‘ä½¿ç”¨Cloudflareç»•è¿‡æœåŠ¡")
                        print("="*60 + "\n")
                    
                    self._adjust_interval(domain, False)
                    self.current_proxy_index += 1
                    request_manager.exit_request(domain)
                    continue
                
                # å¤„ç†429çŠ¶æ€ç 
                if response.status_code == 429:
                    retry_after = min(int(response.headers.get('Retry-After', 10)), 30)
                    self.logger.warning(f"â±ï¸  é‡åˆ°429é™é€Ÿï¼Œç­‰å¾…{retry_after}ç§’")
                    self._adjust_interval(domain, False)
                    time.sleep(retry_after)
                    request_manager.exit_request(domain)
                    continue
                
                # å¤„ç†403çŠ¶æ€ç ï¼ˆéCloudflareï¼‰
                if response.status_code == 403 and not is_cloudflare:
                    self.logger.warning("ğŸ”’ è®¿é—®è¢«ç¦æ­¢ï¼Œå¯èƒ½è§¦å‘åçˆ¬æœºåˆ¶")
                    self._adjust_interval(domain, False)
                    self.current_proxy_index += 1
                    time.sleep(random.uniform(5, 10))
                    request_manager.exit_request(domain)
                    continue
                
                # å¤„ç†500+çŠ¶æ€ç 
                if response.status_code >= 500:
                    self.logger.warning(f"ğŸ”¥ æœåŠ¡å™¨é”™è¯¯ {response.status_code}ï¼Œé‡è¯•ä¸­...")
                    self._adjust_interval(domain, False)
                    time.sleep(random.uniform(3, 8))
                    request_manager.exit_request(domain)
                    continue
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    self._adjust_interval(domain, True)
                    min_len = self.get_min_length_for_domain(domain)
                    html = response.text
                    if self.is_valid_html_for_domain(domain, html):
                        request_manager.record_request(domain, True)
                        if use_cache:
                            page_cache.set(url, html, {
                                'etag': response.headers.get('ETag', ''),
                                'last_modified': response.headers.get('Last-Modified', '')
                            })
                        self.logger.info(f"âœ“ æˆåŠŸè·å–: {url[:50]}...")
                        request_manager.exit_request(domain)
                        return html
                    if len(html) < min_len:
                        short_content_streak += 1
                        snippet = html[:200].replace('\n', ' ')
                        self.logger.warning(f"âš ï¸  å“åº”å†…å®¹è¿‡çŸ­ åŸŸå={domain} å°è¯•={attempt+1}/{max_retries} len={len(html)} æ¬¡æ•°={short_content_streak} ç‰‡æ®µ: {snippet}")
                        if short_content_streak >= 3:
                            request_manager.record_request(domain, False)
                        else:
                            self._adjust_interval(domain, False)
                            force_no_cache = True
                        request_manager.exit_request(domain)
                        continue
                    request_manager.record_request(domain, True)
                    if use_cache:
                        page_cache.set(url, html, {
                            'etag': response.headers.get('ETag', ''),
                            'last_modified': response.headers.get('Last-Modified', '')
                        })
                    self.logger.info(f"âœ“ æˆåŠŸè·å–: {url[:50]}...")
                    request_manager.exit_request(domain)
                    return html
                    
            except requests.exceptions.ProxyError as e:
                self.logger.warning(f"ğŸŒ ä»£ç†è¿æ¥å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
                self._adjust_interval(domain, False)
                request_manager.record_request(domain, False)
                request_manager.exit_request(domain)
                
                # ç§»é™¤å¤±æ•ˆä»£ç†
                if current_proxy and current_proxy in self.proxies and len(self.proxies) > 1:
                    self.proxies.remove(current_proxy)
                    self.logger.warning(f"ğŸ—‘ï¸  ç§»é™¤å¤±æ•ˆä»£ç†ï¼Œå‰©ä½™ {len(self.proxies)} ä¸ªä»£ç†")
                    if len(self.proxies) == 1 and self.proxies[0] is None:
                        self.logger.warning("âš ï¸  æ‰€æœ‰ä»£ç†å¤±æ•ˆï¼Œä»…ä½¿ç”¨ç›´è¿")
                
            except requests.exceptions.ConnectionError as e:
                error_msg = str(e)
                if "ConnectionResetError" in str(type(e).__name__) or "10054" in error_msg:
                    self.logger.warning(f"ğŸ”Œ è¿æ¥è¢«é‡ç½® (å°è¯• {attempt+1}/{max_retries})")
                    
                    # åŒºåˆ†ä¸åŒç±»å‹çš„è¿æ¥é‡ç½®
                    if "10054" in error_msg:
                        print("\n" + "="*60)
                        print("ğŸ”Œ è¿æ¥è¢«è¿œç¨‹ä¸»æœºå¼ºåˆ¶å…³é—­ (é”™è¯¯10054)")
                        print("ğŸ’¡ å¯èƒ½åŸå› :")
                        print("   â€¢ ç›®æ ‡ç½‘ç«™ä¸»åŠ¨æ‹’ç»è¿æ¥")
                        print("   â€¢ ç½‘ç»œé˜²ç«å¢™æ‹¦æˆª")
                        print("   â€¢ ISPé™åˆ¶è®¿é—®")
                        print("   â€¢ ä»£ç†æœåŠ¡å™¨é—®é¢˜")
                        print("="*60 + "\n")
                    
                else:
                    self.logger.warning(f"ğŸ”— è¿æ¥é”™è¯¯: {error_msg}")
                
                self._adjust_interval(domain, False)
                request_manager.record_request(domain, False)
                request_manager.exit_request(domain)
                
            except requests.exceptions.Timeout as e:
                self.logger.warning(f"â° è¯·æ±‚è¶…æ—¶: {str(e)}")
                self._adjust_interval(domain, False)
                request_manager.record_request(domain, False)
                request_manager.exit_request(domain)
                
            except Exception as e:
                self.logger.error(f"â— æœªçŸ¥é”™è¯¯: {type(e).__name__}: {str(e)}")
                self._adjust_interval(domain, False)
                request_manager.record_request(domain, False)
                request_manager.exit_request(domain)
        
        self.logger.error(f"ğŸš« è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•è·å–é¡µé¢å†…å®¹")
        print("\n" + "="*60)
        print("ğŸš« æ‰€æœ‰é‡è¯•å¤±è´¥")
        print("ğŸ’¡ æœ€ç»ˆå»ºè®®:")
        print("   1. ä½¿ç”¨ä»˜è´¹ä»£ç†æœåŠ¡ (ä½å®…ä»£ç† > æ•°æ®ä¸­å¿ƒä»£ç†)")
        print("   2. åˆ‡æ¢åˆ°çœŸå®æµè§ˆå™¨è‡ªåŠ¨åŒ– (Selenium/Playwright)")
        print("   3. é™ä½è¯·æ±‚é¢‘ç‡åˆ°æ¯è¯·æ±‚é—´éš”60ç§’ä»¥ä¸Š")
        print("   4. è€ƒè™‘ä½¿ç”¨Cloudflareç»•è¿‡APIæœåŠ¡")
        print("="*60 + "\n")
        return None

    def parse_video_info(self, html: str, base_url: str) -> List[Dict]:
        try:
            self.logger.info("å¼€å§‹è§£æè§†é¢‘ä¿¡æ¯")
            soup = BeautifulSoup(html, 'lxml')
            videos = []
            
            # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘å®¹å™¨ - ä½¿ç”¨å¤šç§é€šç”¨é€‰æ‹©å™¨
            video_containers = []
            
            # ä¼˜å…ˆé€‰æ‹©å™¨åˆ—è¡¨ - åŸºäºå®é™…é¡µé¢ç»“æ„
            selectors = [
                '.col-xs-6.col-md-3',  # hsex.menä¸»é€‰æ‹©å™¨
                '.thumbnail',          # hsex.menå†…éƒ¨å®¹å™¨
                '.video-item',
                '.item',
                '.card',
                '.video-card',
                '.content-item',
                'div[class*="video"]',
                'div[class*="item"]',
                '.gallery-item',
                '.thumb-item'
            ]
            
            # å°è¯•æ¯ä¸ªé€‰æ‹©å™¨
            for selector in selectors:
                containers = soup.select(selector)
                if containers:
                    video_containers = containers
                    self.logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(containers)} ä¸ªè§†é¢‘å®¹å™¨")
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡è§†é¢‘é“¾æ¥æŸ¥æ‰¾å®¹å™¨
            if not video_containers:
                all_links = soup.select('a[href*="video"], a[href*="watch"], a[href*="play"], a[href*="view"]')
                containers = []
                for link in all_links:
                    parent = link.find_parent(['div', 'article', 'section', 'li'])
                    if parent and parent not in containers:
                        containers.append(parent)
                video_containers = containers
                self.logger.debug(f"é€šè¿‡é“¾æ¥æ‰¾åˆ° {len(video_containers)} ä¸ªè§†é¢‘å®¹å™¨")
            
            self.logger.debug(f"æ€»å…±æ‰¾åˆ° {len(video_containers)} ä¸ªè§†é¢‘å®¹å™¨")
            
            for container in video_containers:
                try:
                    # ä½¿ç”¨é€šç”¨æ–¹æ³•æå–ä¿¡æ¯
                    video_id = self._extract_video_id(container)
                    title = self._extract_title(container)
                    thumbnail_url = self._extract_thumbnail(container, base_url)
                    time_text = self._extract_time(container)
                    
                    # æ¸…ç†æ—¶é—´æ–‡æœ¬ï¼Œç§»é™¤è§‚çœ‹æ¬¡æ•°ç­‰ä¿¡æ¯
                    if time_text:
                        time_text = re.sub(r'\d+.*?(æ¬¡è§‚çœ‹|views|æ’­æ”¾|view|Views|æ¬¡æ’­æ”¾)', '', time_text, flags=re.IGNORECASE).strip()
                        if not time_text:
                            time_text = 'æœ€è¿‘æ›´æ–°'
                    else:
                        time_text = 'æœ€è¿‘æ›´æ–°'
                    
                    video_info = {
                        'video_id': video_id,
                        'title': title,
                        'thumbnail_url': thumbnail_url,
                        'relative_time': time_text,
                        'upload_time': self._parse_relative_time(time_text)
                    }
                    
                    self.logger.debug(f"è§£æåˆ°è§†é¢‘ä¿¡æ¯: {video_info}")
                    
                    # åªè¦æœ‰æ ‡é¢˜å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼ˆé€‚é…hsex.menå¯èƒ½ä¸éœ€è¦video_idï¼‰
                    if title:
                        videos.append(video_info)
                        
                except Exception as e:
                    self.logger.error(f"è§£æå•ä¸ªè§†é¢‘é¡¹æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            self.logger.info(f"æˆåŠŸè§£æ {len(videos)} ä¸ªè§†é¢‘ä¿¡æ¯")
            return videos
            
        except Exception as e:
            self.logger.error(f"è§£æè§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            return []

    def _extract_video_id(self, item) -> str:
        try:
            # ä»è§†é¢‘é“¾æ¥ä¸­æå–ID
            video_link = None
            # å°è¯•å¤šç§å¯èƒ½çš„é“¾æ¥é€‰æ‹©å™¨
            for selector in ['a[href*="video"]', 'a[href*="watch"]', 'a[href*="play"]', 'a[href*="view"]', 'a[href*="movie"]']:
                video_link = item.select_one(selector)
                if video_link:
                    break
            
            if video_link and 'href' in video_link.attrs:
                href = video_link['href']
                self.logger.debug(f"æ‰¾åˆ°è§†é¢‘é“¾æ¥: {href}")
                
                # å°è¯•å¤šç§IDæå–æ¨¡å¼ï¼Œé€‚é…hsex.menå’Œé€šç”¨æ ¼å¼
                patterns = [
                    r'video-(\d+)\.htm',           # hsex.menæ ¼å¼: video-12345.htm
                    r'video/(\d+)',                # /video/12345
                    r'watch\?(?:.*&)?v=(\w+)',     # watch?v=abc123
                    r'play/(\d+)',                  # /play/12345
                    r'movie/(\d+)',                 # /movie/12345
                    r'id=(\d+)',                    # ?id=12345
                    r'/(\d+)(?:/|$)',               # çº¯æ•°å­—ID
                    r'[?&]v=([^&]+)',                # URLå‚æ•°ä¸­çš„vå€¼
                    r'embed/(\w+)',                  # embed/abc123
                    r'v/(\w+)',                      # /v/abc123
                    r'view/(\d+)'                    # /view/12345
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, href)
                    if match:
                        video_id = match.group(1)
                        self.logger.debug(f"æå–åˆ°è§†é¢‘ID: {video_id}")
                        return video_id
                        
                # å¦‚æœURLæ˜¯æ•°å­—ç»“å°¾ï¼Œå°è¯•æå–
                numeric_match = re.search(r'(\d+)(?:\.\w+)?$', href)
                if numeric_match:
                    video_id = numeric_match.group(1)
                    self.logger.debug(f"æå–åˆ°æ•°å­—ID: {video_id}")
                    return video_id
                    
            return ''
        except Exception as e:
            self.logger.error(f"æå–è§†é¢‘IDå¤±è´¥: {str(e)}")
            return ''

    def _extract_title(self, item) -> str:
        try:
            # åŸºäºhsex.menç½‘ç«™ç»“æ„çš„æ ‡é¢˜é€‰æ‹©å™¨
            selectors = [
                '.title h5 a',           # hsex.menä¸»æ ‡é¢˜é€‰æ‹©å™¨
                '.title a',              # hsex.menå¤‡ç”¨æ ‡é¢˜é€‰æ‹©å™¨
                '.video-title',
                '.title',
                'h3',
                'a[title]',
                '.item-title',
                '.video-name',
                '.name',
                '.description',
                'p',
                'a'
            ]
            
            for selector in selectors:
                title_elem = item.select_one(selector)
                if title_elem:
                    # å°è¯•ä¸åŒçš„å±æ€§è·å–æ ‡é¢˜
                    for attr in ['title', 'alt', 'data-title']:
                        if attr in title_elem.attrs:
                            title = title_elem[attr].strip()
                            if title:
                                self.logger.debug(f"ä»å±æ€§ {attr} æ‰¾åˆ°æ ‡é¢˜: {title}")
                                return title
                    
                    # è·å–æ–‡æœ¬å†…å®¹
                    title = title_elem.text.strip()
                    if title:
                        self.logger.debug(f"ä»æ–‡æœ¬æ‰¾åˆ°æ ‡é¢˜: {title}")
                        return title
            
            return ''
        except Exception as e:
            self.logger.error(f"æå–æ ‡é¢˜å¤±è´¥: {str(e)}")
            return ''

    def _extract_thumbnail(self, item, base_url: str) -> str:
        try:
            # å°è¯•ä»imgæ ‡ç­¾æå–
            img = item.select_one('img[src]')
            if img and 'src' in img.attrs:
                src = img['src']
                if src.startswith('//'):
                    return f'https:{src}'
                elif src.startswith('/'):
                    return urljoin(base_url, src)
                return src
            
            # å°è¯•ä»background-imageæ ·å¼æå–
            image_div = item.select_one('.image')
            if image_div:
                style = image_div.get('style', '')
                match = re.search(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
                if match:
                    url = match.group(1)
                    if url.startswith('//'):
                        return f'https:{url}'
                    elif url.startswith('/'):
                        return urljoin(base_url, url)
                    return url
            
            return ''
        except Exception as e:
            self.logger.error(f"Error extracting thumbnail: {str(e)}")
            return ''

    def _extract_time(self, item) -> str:
        try:
            # åŸºäºhsex.menç½‘ç«™ç»“æ„çš„æ—¶é—´é€‰æ‹©å™¨
            time_elem = (
                item.select_one('.info p') or            # hsex.menä¸Šä¼ æ—¶é—´é€‰æ‹©å™¨
                item.select_one('.upload-time') or
                item.select_one('.time') or
                item.select_one('time') or
                item.select_one('.date')
            )
            
            if time_elem:
                text = time_elem.text.strip()
                # æå–æ—¶é—´ä¿¡æ¯ï¼ˆå¦‚"1æœˆå‰"ã€"2æœˆå‰"ï¼‰
                import re
                time_match = re.search(r'(\d+(?:\.\d+)?[kK]?æ¬¡è§‚çœ‹\s+)(.+)', text)
                if time_match:
                    return time_match.group(2).strip()
                return text
            return ''
        except Exception as e:
            self.logger.error(f"Error extracting time: {str(e)}")
            return ''

    def _parse_relative_time(self, time_str: str) -> datetime:
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡"""
        try:
            if not time_str or time_str == 'æœ€è¿‘æ›´æ–°':
                return datetime.now()

            self.logger.debug(f"è§£ææ—¶é—´å­—ç¬¦ä¸²: {time_str}")
            
            # æ¸…ç†å­—ç¬¦ä¸²ï¼Œç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦
            time_str = ' '.join(time_str.split())
            time_str = re.sub(r'[^\w\s\-:]+', '', time_str).strip()
            
            # é¦–å…ˆå°è¯•è§£æå®Œæ•´æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # YYYY-MM-DD
                r'(\d{1,2})-(\d{1,2})-(\d{4})',  # DD-MM-YYYY
                r'(\d{1,2})/(\d{1,2})/(\d{4})',   # MM/DD/YYYY
                r'(\d{4})/(\d{1,2})/(\d{1,2})'    # YYYY/MM/DD
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, time_str)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        # æ ¹æ®æ ¼å¼ç¡®å®šå¹´æœˆæ—¥
                        if pattern.startswith(r'(\d{4})'):
                            year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                        elif pattern.startswith(r'(\d{1,2})-(\d{1,2})-(\d{4})'):
                            day, month, year = int(groups[0]), int(groups[1]), int(groups[2])
                        else:
                            month, day, year = int(groups[0]), int(groups[1]), int(groups[2])
                        
                        try:
                            return datetime(year, month, day)
                        except ValueError:
                            continue

            # è§£æç›¸å¯¹æ—¶é—´ - è‹±æ–‡æ ¼å¼
            now = datetime.now()
            
            # è‹±æ–‡æ—¶é—´æ ¼å¼
            patterns = [
                (r'(\d+)\s*day[s]?\s*ago', 'days'),
                (r'(\d+)\s*hour[s]?\s*ago', 'hours'),
                (r'(\d+)\s*minute[s]?\s*ago', 'minutes'),
                (r'(\d+)\s*week[s]?\s*ago', 'weeks'),
                (r'(\d+)\s*month[s]?\s*ago', 'months'),
                (r'(\d+)\s*year[s]?\s*ago', 'years'),
                # ä¸­æ–‡æ ¼å¼
                (r'(\d+)\s*å¤©å‰', 'days'),
                (r'(\d+)\s*å°æ—¶å‰', 'hours'),
                (r'(\d+)\s*åˆ†é’Ÿå‰', 'minutes'),
                (r'(\d+)\s*å‘¨å‰', 'weeks'),
                (r'(\d+)\s*æœˆå‰', 'months'),
                (r'(\d+)\s*å¹´å‰', 'years')
            ]
            
            for pattern, unit in patterns:
                match = re.search(pattern, time_str, re.IGNORECASE)
                if match:
                    value = int(match.group(1))
                    if unit == 'days':
                        return now - timedelta(days=value)
                    elif unit == 'hours':
                        return now - timedelta(hours=value)
                    elif unit == 'minutes':
                        return now - timedelta(minutes=value)
                    elif unit == 'weeks':
                        return now - timedelta(weeks=value)
                    elif unit == 'months':
                        return now - timedelta(days=30*value)
                    elif unit == 'years':
                        return now - timedelta(days=365*value)
            
            # å°è¯•è§£æè‹±æ–‡æœˆä»½æ ¼å¼
            month_patterns = [
                r'(\w+)\s+(\d{1,2}),?\s+(\d{4})',  # Month DD, YYYY
                r'(\d{1,2})\s+(\w+)\s+(\d{4})',  # DD Month YYYY
            ]
            
            month_map = {
                'jan': 1, 'january': 1, 'feb': 2, 'february': 2, 'mar': 3, 'march': 3,
                'apr': 4, 'april': 4, 'may': 5, 'jun': 6, 'june': 6,
                'jul': 7, 'july': 7, 'aug': 8, 'august': 8,
                'sep': 9, 'september': 9, 'oct': 10, 'october': 10,
                'nov': 11, 'november': 11, 'dec': 12, 'december': 12
            }
            
            for pattern in month_patterns:
                match = re.search(pattern, time_str, re.IGNORECASE)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        # è§£ææœˆä»½
                        month_str = groups[0].lower() if groups[0].isalpha() else groups[1].lower()
                        month = month_map.get(month_str, 1)
                        
                        # è§£ææ—¥å’Œå¹´
                        if groups[0].isdigit():
                            day = int(groups[0])
                            year = int(groups[2])
                        else:
                            day = int(groups[1])
                            year = int(groups[2])
                        
                        try:
                            return datetime(year, month, day)
                        except ValueError:
                            continue
            
            self.logger.warning(f"æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: {time_str}")
            return datetime.now()
            
        except Exception as e:
            self.logger.error(f"è§£ææ—¶é—´å¤±è´¥: {str(e)}")
            return datetime.now()
