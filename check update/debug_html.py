#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTMLå†…å®¹è°ƒè¯•è„šæœ¬ - å¢å¼ºç‰ˆ
ç”¨äºåˆ†æå®é™…è·å–çš„é¡µé¢å†…å®¹ï¼ŒåŒ…æ‹¬å“åº”å¤´ä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import gzip
import io
import re

def debug_response_details():
    """è°ƒè¯•å“åº”è¯¦ç»†ä¿¡æ¯"""
    print("ğŸ” å¼€å§‹è°ƒè¯•å“åº”è¯¦ç»†ä¿¡æ¯...")
    
    # æµ‹è¯•URL
    test_url = "https://hsex.men/user.htm?author=345061255"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0'
    }
    
    try:
        print(f"ğŸ“¡ è·å–é¡µé¢: {test_url}")
        
        # è·å–å“åº”
        response = requests.get(test_url, headers=headers, timeout=10)
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€: {response.status_code}")
        print(f"ğŸ“Š å“åº”å¤´: {dict(response.headers)}")
        print(f"ğŸ“Š ç¼–ç : {response.encoding}")
        print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('content-type', 'unknown')
        print(f"ğŸ“Š å†…å®¹ç±»å‹: {content_type}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‹ç¼©å†…å®¹
        if 'gzip' in response.headers.get('content-encoding', ''):
            print("ğŸ“Š æ£€æµ‹åˆ°gzipå‹ç¼©")
            # å°è¯•è§£å‹
            try:
                decompressed = gzip.decompress(response.content)
                print(f"ğŸ“Š è§£å‹åé•¿åº¦: {len(decompressed)} å­—èŠ‚")
                content = decompressed.decode('utf-8', errors='ignore')
            except:
                content = response.text
        else:
            content = response.text
        
        # ä¿å­˜åŸå§‹å†…å®¹
        with open('debug_response.txt', 'w', encoding='utf-8') as f:
            f.write(f"Status: {response.status_code}\n")
            f.write(f"Headers: {dict(response.headers)}\n")
            f.write(f"Content-Type: {content_type}\n")
            f.write("-" * 50 + "\n")
            f.write(content)
        
        # ä¿å­˜äºŒè¿›åˆ¶å†…å®¹ç”¨äºåˆ†æ
        with open('debug_response.bin', 'wb') as f:
            f.write(response.content)
        
        print(f"ğŸ“„ å“åº”å·²ä¿å­˜åˆ° debug_response.txt å’Œ debug_response.bin")
        
        # å°è¯•è§£æå†…å®¹
        if 'text/html' in content_type:
            soup = BeautifulSoup(content, 'lxml')
            
            print(f"ğŸ“Š é¡µé¢åŸºæœ¬ä¿¡æ¯:")
            print(f"   æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            print(f"   bodyæ ‡ç­¾: {'å­˜åœ¨' if soup.body else 'ä¸å­˜åœ¨'}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Cloudflareé¡µé¢
            if 'cloudflare' in content.lower() or 'ray id' in content.lower():
                print("âš ï¸  æ£€æµ‹åˆ°Cloudflareé˜²æŠ¤é¡µé¢")
                
                # æŸ¥æ‰¾Ray ID
                ray_match = re.search(r'Ray ID: ([a-f0-9]+)', content)
                if ray_match:
                    print(f"   Ray ID: {ray_match.group(1)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç™»å½•é¡µé¢
            if 'login' in content.lower() or 'sign in' in content.lower():
                print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•é¡µé¢")
            
            # æ‰“å°å‰500å­—ç¬¦çš„å†…å®¹é¢„è§ˆ
            preview = content[:500]
            print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {preview}")
            
            # åˆ†æé¡µé¢ç»“æ„
            if soup.body:
                print(f"ğŸ“Š é¡µé¢ç»“æ„:")
                print(f"   æ‰€æœ‰æ ‡ç­¾: {len(soup.find_all())}")
                print(f"   divæ ‡ç­¾: {len(soup.find_all('div'))}")
                print(f"   aæ ‡ç­¾: {len(soup.find_all('a'))}")
                print(f"   imgæ ‡ç­¾: {len(soup.find_all('img'))}")
                
                # æŸ¥æ‰¾å¯èƒ½çš„è§†é¢‘å®¹å™¨
                video_selectors = [
                    'div[class*="video"]',
                    'div[class*="item"]',
                    'a[href*="video"]',
                    'a[href*="watch"]',
                    'img[src*="jpg"]',
                    'img[src*="png"]'
                ]
                
                for selector in video_selectors:
                    elements = soup.select(selector)
                    if elements:
                        print(f"   âœ… {selector}: {len(elements)} ä¸ª")
        else:
            print("âŒ è¿”å›å†…å®¹ä¸æ˜¯HTML")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def test_different_urls():
    """æµ‹è¯•ä¸åŒçš„URLæ ¼å¼"""
    print("\n" + "="*60)
    print("ğŸ”— æµ‹è¯•ä¸åŒURLæ ¼å¼...")
    
    urls = [
        "https://hsex.men",
        "https://hsex.men/",
        "https://hsex.men/user.htm?author=345061255",
        "https://hsex.men/user/345061255",
        "https://hsex.men/profile/345061255"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for url in urls:
        try:
            print(f"\nğŸ“¡ æµ‹è¯•: {url}")
            response = requests.get(url, headers=headers, timeout=5)
            print(f"   çŠ¶æ€ç : {response.status_code}")
            print(f"   å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯HTML
            if 'text/html' in response.headers.get('content-type', ''):
                soup = BeautifulSoup(response.text, 'lxml')
                print(f"   æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
                print(f"   divæ•°é‡: {len(soup.find_all('div'))}")
            else:
                print(f"   å†…å®¹ç±»å‹: {response.headers.get('content-type')}")
                
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    debug_response_details()
    test_different_urls()
    
    print("\n" + "="*60)
    print("ğŸ’¡ è°ƒè¯•å®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„ debug_response.txt å’Œ debug_response.bin")
    print("ğŸ” åˆ†æå“åº”å†…å®¹ç±»å‹å’Œç»“æ„")
    print("="*60)