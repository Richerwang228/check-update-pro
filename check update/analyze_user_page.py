#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æç”¨æˆ·é¡µé¢HTMLç»“æ„
ç”¨äºæ‰¾åˆ°æ­£ç¡®çš„è§†é¢‘é€‰æ‹©å™¨
"""

import requests
from bs4 import BeautifulSoup
import re
import json

def analyze_user_page():
    """åˆ†æç”¨æˆ·é¡µé¢çš„è§†é¢‘ç»“æ„"""
    print("ğŸ” åˆ†æç”¨æˆ·é¡µé¢è§†é¢‘ç»“æ„...")
    
    # æµ‹è¯•URL
    test_url = "https://hsex.men/user.htm?author=345061255"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'identity',  # ç¦ç”¨å‹ç¼©
        'Connection': 'keep-alive',
    }
    
    try:
        print(f"ğŸ“¡ è·å–é¡µé¢: {test_url}")
        response = requests.get(test_url, headers=headers, timeout=10)
        
        print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")
        print(f"ğŸ“Š ç¼–ç : {response.encoding}")
        print(f"ğŸ“Š å†…å®¹ç±»å‹: {response.headers.get('content-type', 'unknown')}")
        
        # ç›´æ¥ä½¿ç”¨response.textï¼Œrequestsä¼šè‡ªåŠ¨å¤„ç†ç¼–ç 
        html = response.text
        
        # ä¿å­˜åŸå§‹HTML
        with open('user_page.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("ğŸ“„ HTMLå·²ä¿å­˜åˆ° user_page.html")
        
        # è§£æHTML
        soup = BeautifulSoup(html, 'lxml')
        
        print(f"\nğŸ“Š é¡µé¢åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
        print(f"   æ‰€æœ‰æ ‡ç­¾: {len(soup.find_all())}")
        print(f"   divæ ‡ç­¾: {len(soup.find_all('div'))}")
        print(f"   aæ ‡ç­¾: {len(soup.find_all('a'))}")
        print(f"   imgæ ‡ç­¾: {len(soup.find_all('img'))}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„è§†é¢‘å®¹å™¨
        print(f"\nğŸ” æŸ¥æ‰¾è§†é¢‘å®¹å™¨...")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è§†é¢‘ç›¸å…³å…ƒç´ 
        video_patterns = [
            # é€šè¿‡classæŸ¥æ‰¾
            'div[class*="video"]',
            'div[class*="item"]',
            'div[class*="content"]',
            'div[class*="card"]',
            'div[class*="box"]',
            'div[class*="list"]',
            
            # å…·ä½“classåç§°
            '.video-item',
            '.item',
            '.content-item',
            '.card',
            '.video-card',
            '.video-box',
            '.video-list-item',
            '.gallery-item',
            '.thumb-item',
            
            # é€šè¿‡IDæŸ¥æ‰¾
            'div[id*="video"]',
            'div[id*="item"]',
            
            # é€šè¿‡å±æ€§æŸ¥æ‰¾
            'div[data-id]',
            'div[data-video]',
        ]
        
        found_containers = []
        for pattern in video_patterns:
            elements = soup.select(pattern)
            if elements:
                found_containers.append({
                    'selector': pattern,
                    'count': len(elements),
                    'elements': elements[:3]  # ä¿å­˜å‰3ä¸ªç”¨äºåˆ†æ
                })
                print(f"   âœ… {pattern}: {len(elements)} ä¸ªå…ƒç´ ")
        
        # åˆ†ææ‰¾åˆ°çš„å…ƒç´ ç»“æ„
        print(f"\nğŸ“‹ åˆ†æå…ƒç´ ç»“æ„...")
        for container in found_containers:
            print(f"\nğŸ“Š é€‰æ‹©å™¨: {container['selector']} ({container['count']}ä¸ª)")
            for i, elem in enumerate(container['elements']):
                print(f"   å…ƒç´  {i+1}:")
                print(f"      æ ‡ç­¾: {elem.name}")
                print(f"      class: {elem.get('class', [])}")
                print(f"      id: {elem.get('id', 'æ— ')}")
                
                # æŸ¥æ‰¾å­å…ƒç´ ä¸­çš„é“¾æ¥å’Œå›¾ç‰‡
                links = elem.find_all('a', href=True)
                imgs = elem.find_all('img', src=True)
                
                if links:
                    print(f"      é“¾æ¥: {len(links)} ä¸ª")
                    for link in links[:2]:
                        print(f"         href: {link['href']}")
                        print(f"         text: {link.get_text(strip=True)[:50]}")
                
                if imgs:
                    print(f"      å›¾ç‰‡: {len(imgs)} ä¸ª")
                    for img in imgs[:2]:
                        print(f"         src: {img['src']}")
                        print(f"         alt: {img.get('alt', 'æ— alt')}")
        
        # æŸ¥æ‰¾ç‰¹å®šçš„è§†é¢‘é“¾æ¥æ¨¡å¼
        print(f"\nğŸ”— æŸ¥æ‰¾è§†é¢‘é“¾æ¥...")
        video_links = soup.find_all('a', href=re.compile(r'video|watch|play|view'))
        print(f"   æ‰¾åˆ° {len(video_links)} ä¸ªå¯èƒ½çš„è§†é¢‘é“¾æ¥")
        
        for i, link in enumerate(video_links[:5]):
            print(f"   {i+1}. href: {link.get('href', 'æ— ')}")
            print(f"      text: {link.get_text(strip=True)[:50]}")
            print(f"      parent class: {link.parent.get('class', []) if link.parent else 'æ— '}")
        
        # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
        print(f"\nğŸ–¼ï¸ æŸ¥æ‰¾å›¾ç‰‡...")
        images = soup.find_all('img', src=True)
        print(f"   æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
        
        # è¿‡æ»¤å¯èƒ½çš„è§†é¢‘ç¼©ç•¥å›¾
        video_thumbnails = []
        for img in images:
            src = img.get('src', '')
            if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):
                video_thumbnails.append({
                    'src': src,
                    'alt': img.get('alt', ''),
                    'parent': img.parent.name if img.parent else None
                })
        
        print(f"   æ‰¾åˆ° {len(video_thumbnails)} å¼ å¯èƒ½çš„è§†é¢‘ç¼©ç•¥å›¾")
        for i, img in enumerate(video_thumbnails[:5]):
            print(f"   {i+1}. src: {img['src']}")
            print(f"      alt: {img['alt']}")
        
        # æŸ¥æ‰¾æ‰€æœ‰divå¹¶åˆ†æå…¶ç»“æ„
        print(f"\nğŸ“‹ åˆ†ææ‰€æœ‰divç»“æ„...")
        all_divs = soup.find_all('div')
        
        # ç»Ÿè®¡classå‡ºç°é¢‘ç‡
        class_counts = {}
        for div in all_divs:
            classes = div.get('class', [])
            for cls in classes:
                class_counts[cls] = class_counts.get(cls, 0) + 1
        
        # æ˜¾ç¤ºæœ€å¸¸è§çš„class
        sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
        print("   æœ€å¸¸è§çš„div class:")
        for cls, count in sorted_classes[:15]:
            print(f"      {cls}: {count}æ¬¡")
        
        # å°è¯•æå–è§†é¢‘ä¿¡æ¯
        print(f"\nğŸ¯ å°è¯•æå–è§†é¢‘ä¿¡æ¯...")
        
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„æå–
        videos = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«è§†é¢‘é“¾æ¥çš„å®¹å™¨
        # å…ˆæŸ¥æ‰¾æ‰€æœ‰åŒ…å«/video/çš„é“¾æ¥
        video_links = soup.find_all('a', href=re.compile(r'/video/\d+'))
        print(f"   æ‰¾åˆ° {len(video_links)} ä¸ªè§†é¢‘é“¾æ¥")
        
        for link in video_links:
            video_info = {}
            
            # æå–è§†é¢‘ID
            href = link.get('href', '')
            match = re.search(r'/video/(\d+)', href)
            if match:
                video_info['video_id'] = match.group(1)
            
            # æŸ¥æ‰¾å®¹å™¨ï¼ˆå‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„divï¼‰
            container = link.find_parent('div')
            if container:
                # åœ¨å®¹å™¨å†…æŸ¥æ‰¾æ ‡é¢˜
                title_elem = (container.find('a', title=True) or 
                            container.find('h3') or 
                            container.find('div', class_=re.compile(r'title')) or
                            container.find('span', class_=re.compile(r'title')) or
                            link)
                
                if title_elem:
                    video_info['title'] = title_elem.get_text(strip=True)
                
                # æŸ¥æ‰¾ç¼©ç•¥å›¾
                img_elem = container.find('img', src=True)
                if img_elem:
                    video_info['thumbnail'] = img_elem['src']
                
                # æŸ¥æ‰¾æ—¶é•¿
                duration_elem = container.find('span', class_=re.compile(r'time|duration'))
                if duration_elem:
                    video_info['duration'] = duration_elem.get_text(strip=True)
            
            if video_info:
                videos.append(video_info)
        
        # æ–¹æ³•2: æŸ¥æ‰¾å…·æœ‰ç‰¹å®šclassçš„div
        # æ ¹æ®æœ€å¸¸è§çš„classå°è¯•
        likely_classes = [cls for cls, count in sorted_classes if count > 1][:5]
        
        for cls in likely_classes:
            elements = soup.find_all('div', class_=cls)
            for elem in elements:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘ç›¸å…³å…ƒç´ 
                video_link = elem.find('a', href=re.compile(r'/video/\d+'))
                if video_link:
                    video_info = {}
                    
                    # æå–è§†é¢‘ID
                    match = re.search(r'/video/(\d+)', video_link['href'])
                    if match:
                        video_info['video_id'] = match.group(1)
                    
                    # æå–æ ‡é¢˜
                    title_elem = elem.find('a', title=True) or elem.find('h3')
                    if title_elem:
                        video_info['title'] = title_elem.get_text(strip=True)
                    
                    # æå–ç¼©ç•¥å›¾
                    img_elem = elem.find('img')
                    if img_elem:
                        video_info['thumbnail'] = img_elem['src']
                    
                    # é¿å…é‡å¤
                    if video_info and video_info not in videos:
                        videos.append(video_info)
        
        print(f"   æå–åˆ° {len(videos)} ä¸ªè§†é¢‘ä¿¡æ¯")
        for i, video in enumerate(videos[:5]):
            print(f"   {i+1}. æ ‡é¢˜: {video.get('title', 'æ— æ ‡é¢˜')}")
            print(f"      ID: {video.get('video_id', 'æ— ID')}")
            print(f"      ç¼©ç•¥å›¾: {video.get('thumbnail', 'æ— ç¼©ç•¥å›¾')}")
            print(f"      æ—¶é•¿: {video.get('duration', 'æ— æ—¶é•¿')}")
        
        return found_containers, videos
        
    except Exception as e:
        print(f"âŒ åˆ†æå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def create_updated_selectors():
    """åŸºäºåˆ†æç»“æœåˆ›å»ºæ›´æ–°çš„é€‰æ‹©å™¨"""
    print("\n" + "="*60)
    print("ğŸ”„ åˆ›å»ºæ›´æ–°çš„é€‰æ‹©å™¨...")
    
    try:
        with open('user_page.html', 'r', encoding='utf-8') as f:
            html = f.read()
        
        soup = BeautifulSoup(html, 'lxml')
        
        # åŸºäºå®é™…é¡µé¢ç»“æ„çš„é€‰æ‹©å™¨
        updated_selectors = [
            'div.video-item',
            'div.item',
            'div.content-item',
            'div[class*="video-item"]',
            'div[class*="video-box"]',
            'a[href*="/video/"]',
            'div[class*="list"] > div',
            'div[data-video-id]',
        ]
        
        print("   å»ºè®®çš„æ›´æ–°é€‰æ‹©å™¨:")
        for selector in updated_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ ")
            else:
                print(f"   âŒ {selector}: æœªæ‰¾åˆ°")
        
        return updated_selectors
        
    except FileNotFoundError:
        print("   âŒ è¯·å…ˆè¿è¡Œ analyze_user_page()")
        return []

if __name__ == "__main__":
    containers, videos = analyze_user_page()
    selectors = create_updated_selectors()
    
    print("\n" + "="*60)
    print("ğŸ’¡ åˆ†æå®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ user_page.html äº†è§£é¡µé¢ç»“æ„")
    print("ğŸ” æ ¹æ®åˆ†æç»“æœæ›´æ–°è§£æé€‰æ‹©å™¨")
    print("="*60)